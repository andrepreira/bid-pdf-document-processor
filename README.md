# Bid PDF Document Processor

**Production-ready ETL pipeline for extracting structured data from construction bid PDFs**  
*Edgevanta Data Engineering Challenge - January 2026*

---

## üèÜ Results Summary

- ‚úÖ **96% Success Rate** - 96/100 PDFs processed successfully
- ‚ö° **14.3 docs/second** - High-throughput processing
- üìä **71.9% Completeness** - Average data extraction completeness
- üîç **79.2% Validation** - Business rule compliance rate
- ‚è±Ô∏è **0.073s** - Average processing time per document

## üéØ Project Overview

This project implements a complete ETL pipeline to process PDF documents from North Carolina DOT construction bid lettings, extracting structured data for analytics and reporting.

**Key Features**:
- Multi-strategy extraction (Regex + PDFPlumber)
- Automated document classification
- Data quality validation with business rules
- PostgreSQL storage with proper schema design
- Comprehensive observability and logging
- Production-ready code with error handling

## üìä Supported Document Types

1. **Invitation to Bid** - Contract solicitation documents
2. **Bid Tabs** - Tabular bid submissions with pricing
3. **Award Letter** - Contract award notifications
4. **Item C Report** - Bid comparison summaries
5. **Bids As Read** - Raw bid readings (summary lines)
6. **Bid Summary** - Bid summary rollups

## üèóÔ∏è Architecture

```
PDF Files ‚Üí Classifier ‚Üí Extractor (Regex/PDFPlumber) ‚Üí Validator ‚Üí PostgreSQL
                                                                       ‚Üí CSV Export
```

### Key Components

- **Classifier**: Identifies document type by filename and content
- **Extractors**: Specialized parsers for each document type
- **Validators**: Ensure data quality and completeness
- **Loaders**: Save to PostgreSQL or export to CSV

## üöÄ Quick Start

### Prerequisites

- Python 3.10+
- Docker & Docker Compose (for PostgreSQL)

### Installation

```bash
# Clone repository
cd bid-pdf-document-processor

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies (uv)
pip install uv
uv pip install -r requirements.txt

# Copy environment file
cp .env.example .env

# Setup database
docker-compose up -d
```

### Run Pipeline

```bash
# Process all PDFs in source directory
python scripts/run_pipeline.py source/source_files/

# Process specific folder
python scripts/run_pipeline.py source/source_files/2023\ nc\ d1/2023-02-01_nc_d1/

# Save results to JSON
python scripts/run_pipeline.py source/source_files/ --output results.json

# Summary only
python scripts/run_pipeline.py source/source_files/ --summary-only
 
# Load results into PostgreSQL
python scripts/run_pipeline.py source/source_files/ --load-postgres

# Load with custom DB URL
python scripts/run_pipeline.py source/source_files/ --load-postgres --database-url "postgresql://user:pass@host:5432/db"

# Restrict files by glob pattern
python scripts/run_pipeline.py source/source_files/ --pattern "**/*Bid Tabs*.pdf"

# Incremental processing (skip unchanged)
python scripts/run_pipeline.py source/source_files/ --incremental --state-file .pipeline_state.json

# Run via Docker (build + run with Postgres)
# Uses DATABASE_URL and SOURCE_DIR from .env
bash scripts/run_docker_pipeline.sh
```

### Pipeline Parameters

| Parameter | Description |
| --- | --- |
| source_dir | Directory containing PDF files (positional) |
| --pattern | Glob pattern for PDF files (default: **/*.pdf) |
| --output | Output JSON file for results |
| --summary-only | Only print summary statistics |
| --incremental | Skip unchanged files using cached fingerprints |
| --state-file | Optional path for incremental state cache |
| --load-postgres | Load extraction results into PostgreSQL |
| --database-url | PostgreSQL connection string (overrides DATABASE_URL env var) |

### Database Migrations

Migrations run automatically when using `--load-postgres` or the Docker entrypoint.
If you need to run them manually:

```bash
bash scripts/run_migrations.sh
```

## üìÅ Project Structure

```
bid-pdf-document-processor/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ extractors/          # PDF extraction logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_extractor.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bids_as_read_extractor.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bid_summary_extractor.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ invitation_extractor.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bid_tabs_extractor.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ award_letter_extractor.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ item_c_extractor.py
‚îÇ   ‚îú‚îÄ‚îÄ transformers/        # Data transformation
‚îÇ   ‚îú‚îÄ‚îÄ loaders/             # Database/CSV loaders
‚îÇ   ‚îú‚îÄ‚îÄ validators/          # Data quality checks
‚îÇ   ‚îú‚îÄ‚îÄ pipeline/            # Orchestration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ classifier.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ orchestrator.py
‚îÇ   ‚îî‚îÄ‚îÄ models/              # Data models
‚îÇ       ‚îú‚îÄ‚îÄ database_models.py
‚îÇ       ‚îî‚îÄ‚îÄ schemas.py
‚îú‚îÄ‚îÄ tests/                   # Unit tests
‚îú‚îÄ‚îÄ scripts/                 # Executable scripts
‚îú‚îÄ‚îÄ notebooks/               # Jupyter notebooks for analysis
‚îú‚îÄ‚îÄ sql/                     # Database schema
‚îú‚îÄ‚îÄ docs/                    # Documentation
‚îî‚îÄ‚îÄ source/                  # Input PDF files
```

## üîç Extraction Strategies

### 1. Regex Pattern Matching
- **Use case**: Structured text with predictable patterns
- **Pros**: Fast, no external dependencies
- **Cons**: Brittle with format variations

### 2. PDFPlumber Table Extraction
- **Use case**: Tabular data (Bid Tabs)
- **Pros**: Accurate for well-formatted tables
- **Cons**: Requires structured layout

### 3. LLM-based Extraction (Future)
- Planned as an optional fallback for complex/edge cases
- See [docs/LLM_GUIDE.md](docs/LLM_GUIDE.md) for the roadmap

## üìä Database Schema

```sql
contracts      -- Main contract information
  ‚îú‚îÄ‚îÄ bidders      -- Companies that submitted bids
  ‚îî‚îÄ‚îÄ bid_items    -- Line items from bid tabs
  
extraction_logs  -- Pipeline execution tracking
```

## üß™ Testing

```bash
# Run all tests
pytest tests/

# Run with coverage
pytest --cov=src tests/

# Run CI-like test script
bash scripts/run_tests_ci.sh

# Run pipeline via Docker + PostgreSQL
bash scripts/run_docker_pipeline.sh
```

## üß© Scripts

- [scripts/run_pipeline.py](scripts/run_pipeline.py) - Executa o pipeline de extra√ß√£o, com op√ß√µes de sa√≠da e carga no Postgres.
- [scripts/run_demo.py](scripts/run_demo.py) - Demonstra√ß√£o com m√©tricas resumidas.
- [scripts/run_tests_ci.sh](scripts/run_tests_ci.sh) - Testes em modo CI com cobertura.
- [scripts/run_docker_pipeline.sh](scripts/run_docker_pipeline.sh) - Build + run via Docker com PostgreSQL.
- [scripts/run_migrations.sh](scripts/run_migrations.sh) - Executa migrations do Alembic.

## ü§ñ LLM Evaluation (Future)

LLM evaluation tooling is planned for a future release. See
[docs/LLM_GUIDE.md](docs/LLM_GUIDE.md) for the roadmap.

## üìà Data Quality & Validation

- Schema validation using Pydantic
- Business rule checks (totals match, dates valid)
- Confidence scoring per extraction
- Extraction logs for auditing

## üéØ Actual Results (Tested on 100 Real PDFs)

| Metric | Target | Actual | Status |
|--------|--------|--------|--------|
| Success Rate | >90% | 96% | ‚úÖ Exceeded |
| Processing Speed | <5s | 0.073s | ‚úÖ 68x faster |
| Data Completeness | >90% | 71.9% | ‚ö†Ô∏è Good (improving) |
| Throughput | N/A | 14.3 docs/s | ‚úÖ Excellent |

**Document Type Breakdown**:
- Invitation to Bid: 27/27 (100%)
- Bid Tabs: 27/27 (100%)
- Award Letter: 28/28 (100%)
- Item C Report: 14/14 (100%)

## üõ†Ô∏è Development

```bash
# Install dev dependencies (uv)
pip install uv
uv pip install -r requirements.txt

# Run linting
flake8 src/

# Format code
black src/

# Type checking
mypy src/
```

## üìù Completed vs Future Enhancements

### ‚úÖ Completed
- [x] Complete ETL pipeline
- [x] 6 specialized extractors
- [x] PostgreSQL loader implementation
- [x] Data validation layer (business rules)
- [x] Jupyter notebook for analysis
- [x] Comprehensive documentation
- [x] Demo script with metrics
- [x] Structured logging

### üîú Future Enhancements
- [ ] Complete bid items table extraction
- [ ] REST API for on-demand extraction
- [ ] Real-time monitoring dashboard
- [ ] Parallel processing (multiprocessing)
- [ ] CI/CD pipeline (GitHub Actions)
- [ ] Automated tests (pytest)
- [ ] Docker production image
- [ ] LLM fallback for low-confidence cases (future)

## ü§ù Technical Decisions

### Why Python?
- Rich ecosystem for PDF processing (pypdf, pdfplumber)
- Strong data engineering libraries (pandas, SQLAlchemy)
- Easy integration with AI/ML tools

### Why PostgreSQL?
- Robust ACID compliance
- Excellent support for structured data
- Good query performance
- Free and open source

### Why Multiple Extraction Methods?
- Different document types require different approaches
- Fallback strategy ensures resilience
- Allows benchmarking and optimization

## ÔøΩ Documentation

- [ARCHITECTURE.md](docs/ARCHITECTURE.md) - System design and technical decisions
- [SUMMARY.md](docs/SUMMARY.md) - Executive summary and results
- [PRESENTATION_GUIDE.md](docs/PRESENTATION_GUIDE.md) - Technical interview preparation

## üé• Quick Demo

```bash
# Run complete demonstration
python scripts/run_demo.py "source/source_files/2023 nc d1/" --output demo_results.json

# Expected output:
# ‚úì 96/100 documents successfully processed
# ‚úì 96.0% extraction success rate
# ‚úì 71.9% average data completeness
# ‚úì 0.073s average processing time
# ‚úì Validated 96 extractions
```

## üèóÔ∏è Built With

- **Python 3.12** - Core language
- **pypdf + pdfplumber** - PDF extraction
- **PostgreSQL** - Data storage
- **SQLAlchemy** - ORM
- **Pydantic** - Data validation
- **structlog** - Structured logging
- **Jupyter** - Analysis & visualization

## üìß Contact

**Andre Pereira**  
Data Engineer Challenge - Edgevanta  
January 2026

---

**Note**: This is a technical exercise demonstrating production-ready data engineering practices for PDF document processing. The solution prioritizes reliability, maintainability, and scalability.
